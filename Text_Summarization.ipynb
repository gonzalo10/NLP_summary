{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"\n",
    "It’s Back to School time here in Colorado, which means both my son and I will be hanging up the swim shorts and kayak paddles and getting back to more serious business for a while.\n",
    "\n",
    "It has been a slow and endlessly sunny and leisurely summer, and a nice break for both of us, which has been very relaxing and a great time for bonding.\n",
    "\n",
    "But relaxation has its limits. At some point all that Chilling Out fades its way into Complacency, and our natural Human nature starts to work against us, telling us to conserve energy and not really do much of anything. And laziness begets more laziness, and life actually becomes less fun.\n",
    "\n",
    "You can see this effect in our activities. I’ve only completed two blog posts over the entire summer holidays, and together we have put out only two YouTube videos. Spending more time at home and less at the MMM Headquarters squat rack has caused me to lose at least five pounds of leg muscle that I had wanted to keep. Little MM has spent a lot less time practicing on the upright bass and putting out songs, and a lot more time playing video games and getting sucked into the “dank memes” and “Trove” channels on Reddit.\n",
    "\n",
    "It has been a fun break, but as the freshly polished school buses awaken with the sunrise, it will be even more fun to get our own lives cranking into a higher gear as well. And if you’re reading this, it means I am off to a great start!\n",
    "\n",
    "Complacency Is Expensive\n",
    "\n",
    "This laziness was affecting my financial life, and your financial life too. I had let thousands of dollars of uninvested cash build up in my checking account, where it was sitting around earning nothing. My credit card bills had come in, been automatically paid, and filed themselves away without me even reviewing them for fraudulent transactions or wussypants spending on my part. And I had a growing mini-mountain of things I need to do regarding insurance, accounting, and legal stuff in both my personal and business domains.\n",
    "\n",
    "And yet once I got my act together last week, I cleaned up the whole mess and set things straight in less than an hour.\n",
    "\n",
    "It’s not Just Me, it’s You\n",
    "\n",
    "When I talk to friends and family, I notice a common theme: they tend to set up certain “hassle” things once, and then ignore them as long as possible unless some absolute crisis comes along and forces them to make a change.\n",
    "\n",
    "“Oh, I just do all my insurance stuff with Jim Schmidt’s Insurance office downtown, because my parents referred me to him when I first moved out for college.\n",
    "\n",
    "Even better, his wife Jane runs a loan brokerage, so she handles all our family’s mortgage needs!”\n",
    "\n",
    "On this surface, this sounds fun and folksy and like a nice way to do business. And that is exactly the way I like to live: keeping my business relationships as casual and fun as I can. But when it comes to money, complacency can come at a price, so at the bare minimum we should find out exactly what price we are paying.\n",
    "\n",
    "For example, just recently a coworking member came to me and asked for some financial help. And as always, I suggested we start by looking at big recurring expenses. So we dug into the details of her insurance and other major bills streaming in from ol’ Jim and Jane, and found an interesting breakdown:\n",
    "\n",
    "Required liability coverage on a 2010 Subaru Forester: $580 per year\n",
    "Optional collision and comprehensive coverage ($500 deductible): $360 per year\n",
    "Home insurance on a 2000 square foot house ($500 deductible): $1450 per year\n",
    "Mortgage interest on a $300,000 loan at 4.85%:  $14,550 per year\n",
    "Student Loan interest on an old $35,000 student loan at 5.5%: $1925 per year\n",
    "Total: $18,865 per year.\n",
    "\n",
    "It’s no wonder my friend was having financial stress – she had interest and insurance costs that were soaking up half of a reasonable annual budget before she could even buy her first bit of groceries or clothing.\n",
    "\n",
    "So, right there we did a quick round of phone calls and online quotes, and streamlined a bit of the insurance coverage by increasing the deductibles. Within 90 minutes (she did most of the work while I had a beer and swept the floors of the HQ), we had the following new set of options:\n",
    "\n",
    "Subaru liability coverage: $380 per year ($200 savings) through Geico\n",
    "Removal of collision and comprehensive (in the unlikely event of a crash, they could afford to replace the car with less than two months of income) ($360 savings)\n",
    "Home insurance on a 2000 square foot house ($5000 deductible): $650 per year ($800 savings) through Safeco\n",
    "Refinanced mortgage to 3.375% through Credible.com*: $10,125 per year ($4,425 savings)\n",
    "Refinanced Student Loan (also Credible) to 3.85%: $1347 per year ($578 savings)\n",
    "New total expenses: $12,502 ($6363 per year in savings!!)\n",
    "\n",
    "It is hard to even express the importance of what just happened here.  My friend just did two hours of work in total while drinking a glass of wine,  and dropped her annual expenses by over $500 per month, or six thousand dollars per year. And she will of course invest these savings, which will then compound to about to about $86,000 every ten years. \n",
    "\n",
    "Even if she has to do this annual round of phone calls and websites once per year to maintain the best rates on everything, she will be earning about $3150 per hour for this work. Hence the bold title of this article, which you can now see is very conservative.\n",
    "\n",
    "The Optimization Council\n",
    "\n",
    "\n",
    "The first Optimization Council meeting at MMM HQ\n",
    "\n",
    "So you’re convinced. $3150 is enough to get you to pick up the phone, but how do know who to call? Who is going to be your coach if you don’t live near Longmont and thus can’t just join the HQ and have Mr. Money Mustache tell you what to do?\n",
    "\n",
    "The great news is that all of this knowledge already exists, right in your own circle of friends. To extract it, you just need to gather them together and get them to talk about it.\n",
    "\n",
    "Earlier this month, I floated exactly this idea with the members of my coworking space, proposing that we form a group with the witty name “The Optimization Council.”\n",
    "\n",
    "The Council would meet every now and then to talk through life’s biggest expenses and opportunities, and harvest the wisdom of the group so we can all benefit from the best ideas in each category.\n",
    "\n",
    "The response to this idea was overwhelmingly positive. So we called a first “test” meeting earlier this month and a small group of us talked through the first few categories, sharing not just names like “I use Schmidt Insurance”, but details like, “We have $250,000 coverage with a $1,000 deductible and our premium is $589 per year.”\n",
    "\n",
    "The meeting was so lively that we quickly ran out of time, but resolved to meet again soon to figure out more things together. I served as the scribe using a shared google doc – here’s a snapshot of that to give you an idea of our topics:\n",
    "\n",
    "So Yes. There is some thinking and work involved. But there’s also an opportunity to drastically improve your short term cashflow and long-term wealth, and break your friends out of their cautious shell to help them get the same benefits.\n",
    "\n",
    "As we learned long ago in Protecting your Money Mustache from Spendy Friends, most people tend towards complacency, and following along with the group. Which leaves a big gaping void at the top of the pyramid where the leadership role waits unfilled.\n",
    "\n",
    "If you are bold enough to climb into this spot (which really means just sending a few emails and Facebook messages, procuring a box or two of wine, and making a large tray of high-end nachos for your guests), you can all reap the rewards for decades to come.\n",
    "\n",
    "And instead of avoiding this little chore like a hassle, dive into it like a gigantic shower of fun and wealth. After all, this is pretty much the core attitude of Mustachianism Itself.\n",
    "\n",
    "In the comments: we can start our own Optimization Council right here. If you have found a good deal on any of the categories of life, feel free to share a quick summary of your location (state), and details of the company and product/service/price that you found is the best. To avoid spam filtering, please use names but not direct links.\n",
    "\n",
    "A Note about Credible:\n",
    "\n",
    "Watchful readers may have noticed I also mentioned this company on Twitter recently. After a few months of skepticism that the world needed yet another financial company, I was convinced by some conversations with the people running it and a Zoom video of the customer experience from a senior employee, with some very candid commentary on their design choices.\n",
    "\n",
    "I like it because they import the lending models from their large supply of hooked-up finance companies, then run the rate comparisons on their own server rather than farming out your personal information to each separate lender. It saves you from filling out multiple applications when collecting rates, and also saves you from getting on everyone’s spam list (they don’t sell your contact information, which is a rare thing among loan search engines).\n",
    "\n",
    "It was a hard model for them to get going, because the banks naturally want to have your information so they can spam you.  But now that they have a growing presence in the market, lenders are forced to come through Credible to get access to this pool of qualified people. After enough testing with people I knew, I found the experience is worth recommending.\n",
    "\n",
    "So I also signed this blog up with their referral program  – please see my Affiliates philosophy if you are curious or skeptical about how any of that works!\n",
    "\n",
    "With all that said, if you want to try it out, here are the links:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    " \n",
    "sentences = sent_tokenize(passage)    \n",
    "sentences = [expand_contractions(i) for i in sentences]\n",
    "sentences = [re.sub('\\n', '', i) for i in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization is an increasingly popular area within NLP and with the advancements in moderns deep learning, we are consistently seeing newer, more novel approaches. The goal of this article is to compare the results of a few approaches that I found interesting:\n",
    "1. Sentence Scoring based on Word Frequency\n",
    "2. TextRank using Universal Sentence Encoder\n",
    "3. Unsupervised Learning using Skip-Thought Vectors\n",
    "\n",
    "Before moving forward, I wanted to give credit to the outstanding Medium authors/articles who are the foundation for this post and help me learn/implement the Text Summarization techniques below:\n",
    "1. https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "2. https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n",
    "3. https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "\n",
    "Some of the code snippets they've provided will be shown here as well but I encourage you to read through their posts too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Scoring based on Word Freqency (Python 2.7/3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach we will explore is the simplest of the three. Here we assign weights to each word based on the frequency of the word in the passage. For example, if \"Soccer\" occurs 4 times within the passage, it will have a weight of 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_freq_table(text_string):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    \n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    freq_table = {}\n",
    "    \n",
    "    for word in words:\n",
    "        #stem word \n",
    "        word = ps.stem(word)\n",
    "        \n",
    "        #remove stopwords\n",
    "        if word in stopwords_list: \n",
    "            continue\n",
    "        elif word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "            \n",
    "    return freq_table\n",
    "\n",
    "freq_table = create_freq_table(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the weights assigned to each word above, we will create a score for each sentence. At the end of the day, we will be taking the score of the top `N` for the summary. As you'd imagine, just by leveraging the raw score of each sentence, the length of certain sentences will skew the results. This is why will normalize the scores by dividing by the length of each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentences(sentences, freq_table):\n",
    "    \n",
    "    sentence_value = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = len(word_tokenize(sentence))\n",
    "        \n",
    "        for wordValue in freq_table:\n",
    "            \n",
    "            if wordValue.lower() in sentence.lower():                \n",
    "                if sentence in sentence_value:\n",
    "                    sentence_value[sentence] += freq_table[wordValue]\n",
    "                else:\n",
    "                    sentence_value[sentence] = freq_table[wordValue]\n",
    "\n",
    "        sentence_value[sentence] = sentence_value[sentence] // word_count_in_sentence\n",
    "    return sentence_value\n",
    "\n",
    "def find_average_score(sentence_value):\n",
    "    sum_values = 0\n",
    "    \n",
    "    for entry in sentence_value:\n",
    "        sum_values += sentence_value[entry]\n",
    "        \n",
    "    average = int(sum_values/len(sentence_value))\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to create the summary, we will take any sentence that has a score that exceeds a threshold. In this case, the threshold will be the average score for for all of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_value, threshold):\n",
    "    sentence_count = 0\n",
    "    \n",
    "    summary = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence in sentence_value and sentence_value[sentence] > threshold:\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "            \n",
    "    return summary \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If Cristiano Ronaldo didn't exist, would Lionel Messi have to invent him? As appealing as that picture might be, however, it is probably a false one - from Messi's perspective, at least. He might show it in a different way, but Messi is just as competitive as Ronaldo. Rather than goals and personal glory, however, the Argentine's personal drug is trophies. Do Messi and Ronaldo inspire each other? \"Maybe subconsciously in some way they've driven each other on,\" said Rodgers. With the very elite performers, that drive comes from within.\"\n"
     ]
    }
   ],
   "source": [
    "#End to End Run\n",
    "freq_table = create_freq_table(\" \".join(sentences))\n",
    "\n",
    "sentence_scores = score_sentences(sentences, freq_table)\n",
    "\n",
    "threshold = find_average_score(sentence_scores)\n",
    "\n",
    "summary = generate_summary(sentences, sentence_scores, 1.0 * threshold)\n",
    "\n",
    "print(re.sub('\\n','',summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Rank using Universal Sentence Embeddings (Python 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the results generated when using universal sentence embeddings and text rank to generate summaries. Before we jump into the code, let's discuss a few concepts that are critical. \n",
    "\n",
    "**Text Rank**\n",
    "This may sound familiar. This is essentially a derivative of the famous PageRank created by the Google cofounders. In PageRank, they generated a matrix that calculaes the probability that a user will move from one page to another. In the case of TextRank, we generate a cosine similarity matrix where we have the similarity of each sentence to each other.\n",
    "\n",
    "A graph is then generated from this cosine similarity matrix and the pagerank algorithm is applied to this graph and scores are then calculated for each sentence. For more information on the Page Rank algorithm, please use the following resource [pagerank link]\n",
    "\n",
    "**Universal Sentence Embeddings**\n",
    "Without going into too much detail, universal sentence embeddings encode word, sentence and paragraph into semantic vectors. They are trained on Deep Averaging Networks. More details can be found here:\n",
    "\n",
    "https://tfhub.dev/google/universal-sentence-encoder/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'C:\\\\Temp\\\\tfhub_modules\\\\1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47\\\\variables\\\\variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rather than being a better player than Ronaldo, Messi's main motivations - according to the people who are close tohim - are being the best possible version of Lionel Messi, and winning as many trophies as possible. He might show it in a different way, but Messi is just as competitive as Ronaldo. Messi and Ronaldo ferociously competing with each other for everyone else's acclaim is a nice story for fans to debate and the media to spread, but it has / it is probably not particularly true. Do Messi and Ronaldo inspire each other? Ronaldo, it can be said, never looks happy on the field of play unless he has / he is just scored a goal - and even then he has / he is not happy for long, because he just wants to score another one.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "#generate cosine similarity matrix\n",
    "sim_matrix = cosine_similarity(message_embeddings)\n",
    "\n",
    "#create graph and generate scores from pagerank algorithms\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "   \n",
    "num_of_sentences = 5\n",
    "    \n",
    "summary = \" \".join([i[1] for i in ranked_sentences[:num_of_sentences]])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning using Skip Thought Vectors (Python 2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this, in my opinion, is the newest and most novel approach we've discussed here. The high level approach is as follows:\n",
    "\n",
    "Text Cleaning -> Encoder/Decoder -> K Means Clustering -> Extract Sentences Closest to Cluster Center\n",
    "\n",
    "Again, there are two main concepts I want to discuss before jumping into the solution:\n",
    "\n",
    "**Skip Thought Vectors**\n",
    "\n",
    "Here, we use a encoder/decoder framework to generate feature vectors Taking it from Kushal Chauhan's post, here is how the encoder and decoder layers are defined:\n",
    "1. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers.\n",
    "2. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences - S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks.\n",
    "\n",
    "Similar to how Word2Vec embeddings are trained by predicting the surrounding words, the Skip Thought Vectors are trained by predicting the sentence at time, t-1 and t+1. As this model is trained, the learned representation (hidden layer) will now place similar sentences closer together which enables higher performance clustering.\n",
    "\n",
    "I encourage you to review the paper on the same subject for more clarity.\n",
    "\n",
    "**K-Means Clustering**\n",
    "\n",
    "Most of you will be familiar with this form of unsupervised learning but I want to elaborate on how it is used and why it is interesting.\n",
    "\n",
    "As we are aware, each cluster will have some center point which, in the vector space, would indicate the point which closely represents the theme of that cluster. With this in mind, when trying to create a summary, we should only need the sentence which is the closest to the center of that cluster. The key here is choosing the correct number of clusters to do a good job of summarizing the content. Kushal's post recommends that we calculate the cluster size by taking 30% of the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "14\n",
      "34\n",
      "13\n",
      "41\n",
      "19\n",
      "25\n",
      "5\n",
      "11\n",
      "40\n",
      "21\n",
      "22\n",
      "12\n",
      "20\n",
      "7\n",
      "24\n",
      "48\n",
      "18\n",
      "6\n",
      "28\n",
      "33\n",
      "27\n",
      "16\n",
      "32\n",
      "17\n",
      "49\n",
      "10\n",
      "9\n",
      "31\n",
      "35\n",
      "3\n",
      "29\n",
      "53\n",
      "52\n",
      "26\n",
      "8\n",
      "23\n",
      "45\n",
      "42\n",
      "46\n",
      "36\n",
      "54\n",
      "88\n",
      "15\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import skipthoughts\n",
    "\n",
    "# You would need to download pre-trained models first\n",
    "model = skipthoughts.load_model()\n",
    "\n",
    "encoder = skipthoughts.Encoder(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "33\n",
      "6\n",
      "37\n",
      "13\n",
      "9\n",
      "25\n",
      "32\n",
      "43\n",
      "38\n",
      "18\n",
      "16\n",
      "24\n",
      "31\n",
      "28\n",
      "27\n",
      "58\n",
      "34\n",
      "41\n",
      "23\n",
      "30\n",
      "15\n",
      "109\n",
      "29\n",
      "158\n",
      "14\n",
      "36\n",
      "17\n",
      "21\n",
      "35\n",
      "20\n",
      "19\n",
      "71\n",
      "87\n",
      "8\n",
      "55\n",
      "49\n",
      "45\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "encoded =  encoder.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the skipthoughts dependencies can be found here.\n",
    "As mentioned above, the number of clusters will be the number of sentences that will be included in the summary. For this example, we used a cluster size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans = kmeans.fit(encoded)\n",
    "\n",
    "n_clusters = int(np.ceil(len(encoded)**0.6))\n",
    "print(n_clusters)\n",
    "\n",
    "avg = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, encoded)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "summary = ' '.join([sentences[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It has been a fun break, but as the freshly polished school buses awaken with the sunrise, it will be even more fun to get our own lives cranking into a higher gear as well. So we dug into the details of her insurance and other major bills streaming in from ol’ Jim and Jane, and found an interesting breakdown:Required liability coverage on a 2010 Subaru Forester: $580 per yearOptional collision and comprehensive coverage ($500 deductible): $360 per yearHome insurance on a 2000 square foot house ($500 deductible): $1450 per yearMortgage interest on a $300,000 loan at 4.85%:  $14,550 per yearStudent Loan interest on an old $35,000 student loan at 5.5%: $1925 per yearTotal: $18,865 per year. It’s not Just Me, it’s YouWhen I talk to friends and family, I notice a common theme: they tend to set up certain “hassle” things once, and then ignore them as long as possible unless some absolute crisis comes along and forces them to make a change. And I had a growing mini-mountain of things I need to do regarding insurance, accounting, and legal stuff in both my personal and business domains. And yet once I got my act together last week, I cleaned up the whole mess and set things straight in less than an hour. Within 90 minutes (she did most of the work while I had a beer and swept the floors of the HQ), we had the following new set of options:Subaru liability coverage: $380 per year ($200 savings) through GeicoRemoval of collision and comprehensive (in the unlikely event of a crash, they could afford to replace the car with less than two months of income) ($360 savings)Home insurance on a 2000 square foot house ($5000 deductible): $650 per year ($800 savings) through SafecoRefinanced mortgage to 3.375% through Credible.com*: $10,125 per year ($4,425 savings)Refinanced Student Loan (also Credible) to 3.85%: $1347 per year ($578 savings)New total expenses: $12,502 ($6363 per year in savings!!) As we learned long ago in Protecting your Money Mustache from Spendy Friends, most people tend towards complacency, and following along with the group. It is hard to even express the importance of what just happened here. $3150 is enough to get you to pick up the phone, but how do know who to call? To extract it, you just need to gather them together and get them to talk about it. After all, this is pretty much the core attitude of Mustachianism Itself. It was a hard model for them to get going, because the banks naturally want to have your information so they can spam you.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
